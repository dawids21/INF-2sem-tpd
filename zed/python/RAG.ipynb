{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f0811dab-f855-475a-a7ab-2dfce0f5a2be",
      "metadata": {
        "id": "f0811dab-f855-475a-a7ab-2dfce0f5a2be"
      },
      "source": [
        "# Retrieval Augmented Generation\n",
        "\n",
        "Ten notatnik pomoże Ci zapoznać się z podstawowym podejściem do Retrieval Augmented Generation (RAG). W trakcie ćwiczenia będziemy korzystać głównie z bibliotek [google-generativeai](https://github.com/google-gemini/generative-ai-python), [langchain](https://python.langchain.com/) i [trulens](https://www.trulens.org/). Po uzupełnieniu tego notatnika powinieneś wiedzieć:\n",
        "- czym jest RAG,\n",
        "- jak z poziomu kodu komunikować się z LLMem,\n",
        "- jak pobierać i dzielić dokumenty tekstowe na potrzeby RAG,\n",
        "- jak policzyć zanurzenia i przechowywać zanurzenia dla swoich dokumentów,\n",
        "- jak wykonać sematyczne wyszukiwanie wśród dokumentów,\n",
        "- jak wstrzykiwać kontekst do zapytań do LLMów,\n",
        "- jak prowadzić dłuższe konwersacje z LLMem,\n",
        "- jak mierzyć i monitorować jakość odpowiedzi LLMów.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6735ec64-5f8e-42de-903c-2584069a847c",
      "metadata": {
        "id": "6735ec64-5f8e-42de-903c-2584069a847c"
      },
      "source": [
        "## Przygotowanie\n",
        "\n",
        "Do pracy z LLMami potrzebny będzie klucz API do dostawcy takowych. Na tych zajęciach będziemy łączyć się z modelami firmy Google. Jeśli jeszcze nie masz klucza API, skorzystaj z instrukcji na stronie: https://ai.google.dev/gemini-api/docs/api-key.\n",
        "\n",
        "Oprócz tego będziemy potrzebować kilku bibliotek:\n",
        "\n",
        "```{command}\n",
        "pip install google-generativeai langchain langchain-community langchain-google-genai pypdf youtube-transcript-api chromadb trulens trulens-apps-langchain trulens-providers-langchain\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai langchain langchain-community langchain-google-genai pypdf youtube-transcript-api chromadb trulens trulens-apps-langchain trulens-providers-langchain"
      ],
      "metadata": {
        "id": "t55sjsmwFjRh"
      },
      "id": "t55sjsmwFjRh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3da20b87-4213-4184-9ed8-a672b5559454",
      "metadata": {
        "id": "3da20b87-4213-4184-9ed8-a672b5559454"
      },
      "source": [
        "Jeśli masz potrzebne biblioteki, możesz sprawdzić czy jesteś w stanie komunikować się z LLMem. Wykonaj poniższy kod, żeby sprawdzić czy wszystko działa. Jeśli nie masz klucza API w zmiennej systemowej, po prostu wklej go do kodu poniżej zamiast odwołania do zmiennej systemowej. Jeśli chcesz pracować ze zmienną systemową, prawdopodobnie będziesz musiał po dodaniu zmiennej systemowej uruchomić nowy terminal, w którym nowa zmienna będzie widoczna, ponownie odpalić serwer jupyter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a33419a0-a224-4a49-b725-fe8c7940f14a",
      "metadata": {
        "id": "a33419a0-a224-4a49-b725-fe8c7940f14a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "api_key = ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "eQyhxS3jiFqj"
      },
      "id": "eQyhxS3jiFqj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e97f30f0-2627-46ea-a00e-b7ecd923cbb1",
      "metadata": {
        "id": "e97f30f0-2627-46ea-a00e-b7ecd923cbb1"
      },
      "source": [
        "**Zad. 1: Uruchom poniższy kod. Możesz zmodyfikować opis systemu i zapytanie.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8986db69-97a4-4db4-b1b0-d6ac8dbae779",
      "metadata": {
        "id": "8986db69-97a4-4db4-b1b0-d6ac8dbae779"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    system_instruction=\"You are a poetic assistant, skilled in songwriting.\"\n",
        "    )\n",
        "response = model.generate_content(\"Write lyrics for a country song about Polish winters.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "ZvIlgtiYasnn"
      },
      "id": "ZvIlgtiYasnn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "605b30d7-34ba-46de-adca-f2283f69e9cb",
      "metadata": {
        "id": "605b30d7-34ba-46de-adca-f2283f69e9cb"
      },
      "source": [
        "Oprócz informacji zwrotnej od LLMa mamy również szereg metadanych, w tym ile tokenów wysłaliśmy a ile odebraliśmy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a6b6273-459d-49aa-9faa-53016a9f3b20",
      "metadata": {
        "id": "5a6b6273-459d-49aa-9faa-53016a9f3b20"
      },
      "source": [
        "Na koniec zapiszemy sobie treść wyniku, może się jeszcze przyda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9b290a-5cd3-48f7-9479-046bd6cc06ab",
      "metadata": {
        "id": "1a9b290a-5cd3-48f7-9479-046bd6cc06ab"
      },
      "outputs": [],
      "source": [
        "song = response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a5fa74-31cb-4552-aa04-94eda6640b30",
      "metadata": {
        "id": "16a5fa74-31cb-4552-aa04-94eda6640b30"
      },
      "source": [
        "## Wgrywanie danych\n",
        "\n",
        "Langchain (jak i inne narzędzia, np. LlamaIndex) posiadają szereg funkcji pomocniczych do zdobywania tekstu z filmów, podkastów, baz danych, stron internetowych i innych źródeł. Na potrzeby tych zajęć, jako kontekst wykorzystamy dwie krótkie książki Andrew Ng w formacie PDF i napisy z dwóch filmów na Youtube."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b991b89-b0e5-4afb-8524-4d1537e97b35",
      "metadata": {
        "id": "3b991b89-b0e5-4afb-8524-4d1537e97b35"
      },
      "outputs": [],
      "source": [
        "# Wczytujemy książki\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(\"./books\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f761f8-5f35-4ae5-8f29-a0f911fab497",
      "metadata": {
        "id": "b2f761f8-5f35-4ae5-8f29-a0f911fab497"
      },
      "source": [
        "PDFy są domyślnie dzielone na strony, zatem zmienna `docs` to lista stron. Zobaczmy ile stron mają łącznie obie książki i co jest na stronie 23."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbb08129-fead-4a79-a807-65de986b90f0",
      "metadata": {
        "scrolled": true,
        "id": "bbb08129-fead-4a79-a807-65de986b90f0"
      },
      "outputs": [],
      "source": [
        "print(f\"Łacznie w książka jest {len(docs)} stron.\")\n",
        "print()\n",
        "print(f\"Zawartość 23. strony w kolekcji to:\\n{docs[22].page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff312339-0bfd-47f7-8b79-f13ce7c32e5e",
      "metadata": {
        "id": "ff312339-0bfd-47f7-8b79-f13ce7c32e5e"
      },
      "source": [
        "Teraz pobierzemy napisy z dwóch filmików gdzie przemawia Andrew Ng. Zwróć uwagę na parametr language - pozwala nam on priorytetyzować ręcznie sporządzone napisy dostarczone przez twórcę filmu (automatycznie wygenerowane napisy w przypadku pierwszego filmu nie są idealne...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f520c10-16f0-4a8d-9772-6677713d2187",
      "metadata": {
        "id": "9f520c10-16f0-4a8d-9772-6677713d2187"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "clips = []\n",
        "\n",
        "for link in [\"https://www.youtube.com/watch?v=5p248yoa3oE\",\n",
        "             \"https://www.youtube.com/watch?v=0jspaMLxBig\"]:\n",
        "    loader = YoutubeLoader.from_youtube_url(link, add_video_info=False, language=[\"en-US\", \"en-GB\", \"en\"])\n",
        "    clips.extend(loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d35e3d47-fc72-4fd7-bcf8-0f25a701a9c0",
      "metadata": {
        "scrolled": true,
        "id": "d35e3d47-fc72-4fd7-bcf8-0f25a701a9c0"
      },
      "source": [
        "**Zad. 2: Zobacz ile fragmentów (obiektów typu Document) mają łącznie teksty obu filmików i co jest w transkrypcie o indeksie 0.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4e826d-68e3-41cf-9f03-ac192f3790e3",
      "metadata": {
        "id": "0b4e826d-68e3-41cf-9f03-ac192f3790e3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "eb806455-7fc5-46bc-8e50-e1b3b4f31f82",
      "metadata": {
        "id": "eb806455-7fc5-46bc-8e50-e1b3b4f31f82"
      },
      "source": [
        "Na koniec rozszerzemy tekst z książek o transkrypty z filmów, żeby mieć jedną wspólną listę kontekstów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf20590d-c312-4276-b0f7-13c823c62140",
      "metadata": {
        "id": "bf20590d-c312-4276-b0f7-13c823c62140"
      },
      "outputs": [],
      "source": [
        "docs.extend(clips)\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab6594a-f19e-4150-97a3-3ed383c07787",
      "metadata": {
        "id": "dab6594a-f19e-4150-97a3-3ed383c07787"
      },
      "source": [
        "## Dzielenie tekstu na części\n",
        "\n",
        "Na temat dzielenia tekstu na mniejsze fragmenty można by przygotować osobny tutorial. Można dzielić na podstawie znaków, tokenów, parsować zdania za pomocą nltk, wykrywać akapity i rozdziały, tworzyć hierarchie fragmentów. Przykłady bardziej zaawansowanych technik z wykorzystaniem LlamaIndex można znaleźć na [blogach](https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b) i [darmowych kursach](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/).\n",
        "\n",
        "W ramach tych zajęć skorzystamy tylko z jednego prostego podejścia do dzielenia tekstu opartego na wybranych znakach. `RecursiveCharacterTextSplitter`. bo tak nazywa się klasa z której skorzystamy, stara się dzielić tekst na mniejsze części o zadanej długości. W tym celu wyszukuje wskazanych znaków i wybiera pierwszy, który pozwoli uzyskać fragment nie dłuższy niż wskazana liczba znaków. Zobacz jak to działa na przykładzie poniżej."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d1c4409-fd37-4547-a017-3067188cc376",
      "metadata": {
        "id": "4d1c4409-fd37-4547-a017-3067188cc376"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \", \", \" \", \"\"]\n",
        ")\n",
        "r_splitter.split_text(song)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b5e5164-61fd-45f5-bb94-aa3b6ae2efbf",
      "metadata": {
        "id": "8b5e5164-61fd-45f5-bb94-aa3b6ae2efbf"
      },
      "outputs": [],
      "source": [
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=20,\n",
        "    chunk_overlap=10,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \", \", \" \", \"\"]\n",
        ")\n",
        "r_splitter.split_text(song)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7b33ec4-23e9-4184-a543-da256b8c5ed1",
      "metadata": {
        "id": "b7b33ec4-23e9-4184-a543-da256b8c5ed1"
      },
      "source": [
        "**Zad. 3: Podziel dokumenty w zmiennej `docs` na części o długości 750 z zakładką o rozmiarze 150. Możesz do tego użyć fukcji `split_documents` zamiast `split_text`. Wynik przypisz do zmiennej `splits`. Sprawdź ile fragmentów zawiera `splits` i porównaj to z liczbą elementów w `docs`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6e65f4-6fde-40df-bc28-2cda3b8e3330",
      "metadata": {
        "id": "eb6e65f4-6fde-40df-bc28-2cda3b8e3330"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d115fea0-88fe-4371-bb7b-b016c53375e1",
      "metadata": {
        "id": "d115fea0-88fe-4371-bb7b-b016c53375e1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4be0515d-50ba-45b8-b5ae-668f7c17fbc1",
      "metadata": {
        "id": "4be0515d-50ba-45b8-b5ae-668f7c17fbc1"
      },
      "source": [
        "## Tworzenie i przechowywanie zanurzeń"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f0661c-bc88-4b23-bc44-acb8714ce9b9",
      "metadata": {
        "id": "52f0661c-bc88-4b23-bc44-acb8714ce9b9"
      },
      "source": [
        "Jak wiadomo z wykładów, tekst można zapisywać do baz w różnorakich formatach, jednak dominują formaty wektorowe. Pracując z LLMami będzie nam zależeć na wyszukiwaniu semantycznym, czyli opratnym na znaczeniu tekstu a nie na występowaniu konkretnych słów. Użyjemy zanurzeń od Google, ale można równie dobrze korzystać z zanurzeń udostępnianych na HuggingFace (no. BAAI/bge-small-en-v1.5 albo BAAI/bge-large-en-v1.5) i liczyć je lokalnie na komputerze.\n",
        "\n",
        "Wykonaj poniższy kod, aby policzyć zanurzenia dla przykładów z wykładu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0749277-1645-4f60-b04e-1e75197409fc",
      "metadata": {
        "id": "c0749277-1645-4f60-b04e-1e75197409fc"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "embedding = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\"\n",
        "    )\n",
        "\n",
        "sentence1 = \"Ja lubię eksplorację danych.\"\n",
        "sentence2 = \"Ja lubię pływać.\"\n",
        "sentence3 = \"Ja uwielbiam biegać.\"\n",
        "\n",
        "embedding1 = embedding.embed_query(sentence1)\n",
        "embedding2 = embedding.embed_query(sentence2)\n",
        "embedding3 = embedding.embed_query(sentence3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "631eb324-eb7c-40e3-8444-4b3a1f48db35",
      "metadata": {
        "id": "631eb324-eb7c-40e3-8444-4b3a1f48db35"
      },
      "source": [
        "**Zad. 4: Podejrzyj jak wygląda takie zanurzenie i jaką ma długość. Następnie policz odległość między każdą parą zanurzeń. Czy zanurzenia 2 i 3 są do siebie bardziej podobne niż pozostałe pary?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c102fe40-2e67-4572-beaf-665d73ac8c5f",
      "metadata": {
        "id": "c102fe40-2e67-4572-beaf-665d73ac8c5f"
      },
      "outputs": [],
      "source": [
        "print(len(embedding2))\n",
        "\n",
        "print(np.dot(embedding1, embedding2))\n",
        "print(np.dot(embedding1, embedding3))\n",
        "print(np.dot(embedding2, embedding3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ebb65d-6457-499e-becd-ab3ef2d14b81",
      "metadata": {
        "id": "a1ebb65d-6457-499e-becd-ab3ef2d14b81"
      },
      "source": [
        "Gdy już wiemy jak działa zanurzenia i mamy odpowiedni obiekt do tworzenia takowych w zmiennej `embedding`, czas stworzyć bazę danych dla naszych tekstów. Opcji jest wiele, ale my skorzystamy z bazy Chroma, która potrafi działać w pamięci jak i szybko stworzyć małą bazę sqlite lokalnie na dysku.\n",
        "\n",
        "Uruchom poniższy kod, aby stworzyć bazę zanurzeń, zapisać ją na dysk i zobaczyć ile elementów ma w środku."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d411544-863c-4c04-8463-9ddaea4cd667",
      "metadata": {
        "id": "3d411544-863c-4c04-8463-9ddaea4cd667"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "#!rm -rf ./chroma\n",
        "persist_directory = './chroma/'\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53b66f9f-a429-4d43-822d-e8d03a3534d7",
      "metadata": {
        "id": "53b66f9f-a429-4d43-822d-e8d03a3534d7"
      },
      "source": [
        "Po stworzeniu bazy możemy wypróbować wspomniane wyszukiwanie semantyczne. Poniżej napiszemy treść przykładowego zapytania i poprosimy o 3 najbardziej pasujące fragmenty z bazy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc4e99e4-a75d-4bee-81fd-cd8d0d982508",
      "metadata": {
        "id": "bc4e99e4-a75d-4bee-81fd-cd8d0d982508"
      },
      "outputs": [],
      "source": [
        "question = \"What is an eyeball dataset?\"\n",
        "relevant_splits = vectordb.similarity_search(question, k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76859aa7-6fc1-4e37-9f97-f0f4a360ebe5",
      "metadata": {
        "id": "76859aa7-6fc1-4e37-9f97-f0f4a360ebe5"
      },
      "source": [
        "**Zad. 5: Sprawdź ile fragmentów zwróciła baza. Co jest w tych fragmentach? Z których książek lub filmów one pochodzą?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ff0dfb-e3c5-4172-94e6-04c0feafa6bc",
      "metadata": {
        "id": "f3ff0dfb-e3c5-4172-94e6-04c0feafa6bc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "32780ee7-7888-471d-aff7-6c5912186e96",
      "metadata": {
        "id": "32780ee7-7888-471d-aff7-6c5912186e96"
      },
      "source": [
        "Powyżej zapytanie semantyczne zwróciło obiecujące wyniki. Czasami niestety to nie wystarcza. W przypadku duplikatów w bazie, trzeba dbać o różnorodność zwracanych fragmentów. W innych przypadkach trzeba oprócz wyszukiwania semantycznego ograniczyć się do wybranych dokumentów bo są one wprost wskazane w pytaniu. Tymi rzeczami nie mamy czasu się zajmować, ale w zależności od potrzeb można takie problemy rozwiązywać odrobiną technik z tradycyjnych baz danych lub poprosić LLM o pomoc przy zapytaniu do bazy kontekstów."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f49fda04-5cab-4a77-8fd2-261f05466c45",
      "metadata": {
        "id": "f49fda04-5cab-4a77-8fd2-261f05466c45"
      },
      "source": [
        "## Odpowiadanie na pytania z wykorzystaniem kontekstu (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f760051-25b2-4db9-9e67-491fca2d4294",
      "metadata": {
        "id": "8f760051-25b2-4db9-9e67-491fca2d4294"
      },
      "source": [
        "Po zebraniu dokumentów, podzieleniu ich na mniejsze fragmenty, policzeniu zanurzeń i zapisaniu ich do bazy, możemy przejść do RAG. To jest moment w, którym błyszczy langchain. Langchain pozwala tworzyć strumienie wywołań różnych narzędzi i przekazywać wyniki jednego narzędzia jako wejście do innego. W tym wypadku przekażemy zapytanie do bazy zanurzeń a następnie zapytanie wraz fragmentami z bazy przekażemy do LLMa. Taki prosty łańcuch wywołań można stworzyć korzystając z kodu poniżej."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b490dab-d206-4c2c-8259-74de8b375d31",
      "metadata": {
        "id": "5b490dab-d206-4c2c-8259-74de8b375d31"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\") # API zmienia się bardzo szybko i co rusz coś staje się deprecated. Wyciszymy ostrzeżenia.\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,  #Niska temperatura = mało losowości w odpowiedzi LLMa\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ad01df6-b177-4ecd-9bdf-4494a8f4e820",
      "metadata": {
        "id": "8ad01df6-b177-4ecd-9bdf-4494a8f4e820"
      },
      "source": [
        "Mając taki prosty łąncuch wywołań możemy zadać zapytanie do LLMa licząc, że skorzysta z wyszukanego tekstu podczas udzielania odpowiedzi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90594ab-e46e-4354-9c0d-6a3b84089b03",
      "metadata": {
        "scrolled": true,
        "id": "f90594ab-e46e-4354-9c0d-6a3b84089b03"
      },
      "outputs": [],
      "source": [
        "question = \"What is an eyeball dataset?\"\n",
        "\n",
        "result = qa_chain({\"query\": question})\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5da32e88-7dbf-4dfe-b979-49e842f6ed03",
      "metadata": {
        "id": "5da32e88-7dbf-4dfe-b979-49e842f6ed03"
      },
      "source": [
        "_Ciekawscy mogą teraz uruchomić Gemini w osobnym oknie i zobaczyć jak LLM odpowiedziałby na to samo pytanie bez znajomości kontekstu._\n",
        "\n",
        "Rzeczy, które mogą się wydarzyć jest znacznie więcej. Jedną z istotniejszych jest dodanie zdefiniowanego przez nas prompta. Prompt engineering jest sztuką, która potrafi znacząco wpłynąć na działanie RAG. Niekiedy prompty są baaaardzo długie, aby skutecznie nakierować LLM na to o co deweloperowi chodzi. Poniżej, poprosimy o to żeby odpowiedzi były zwięzłe i żeby zawsze kończyły się frazą \"Thanks for asking!\". Ponadto poprosimy o zwracanie dokumentów kontekstowych fraz z odpowiedzią."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c2f665-e213-4e2c-9bfa-0294c164bc70",
      "metadata": {
        "id": "82c2f665-e213-4e2c-9bfa-0294c164bc70"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359e44d0-592f-4200-9172-71fa350750de",
      "metadata": {
        "id": "359e44d0-592f-4200-9172-71fa350750de"
      },
      "source": [
        "**Zad. 6: Ponownie zadaj to samo zapytanie. Jak zmieniła się odpowiedź? Zobacz z jakich książek i stron pochodzi kontekst.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c909170-3a9c-4edd-909b-e90f724fe1eb",
      "metadata": {
        "id": "9c909170-3a9c-4edd-909b-e90f724fe1eb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ecf23826-d893-4bac-8722-c0952571ec60",
      "metadata": {
        "id": "ecf23826-d893-4bac-8722-c0952571ec60"
      },
      "source": [
        "Odpowiedź w istocie jest zwięzła. Poprośmy żeby ją rozwinął."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f25ac3-52c2-42f8-afd7-129b3f40fc3f",
      "metadata": {
        "id": "88f25ac3-52c2-42f8-afd7-129b3f40fc3f"
      },
      "outputs": [],
      "source": [
        "result = qa_chain({\"query\": \"Can you provide a a longer and more detailed response to my last question?\"})\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fddb0d4-ed94-4414-ad72-d13e511d49d8",
      "metadata": {
        "id": "1fddb0d4-ed94-4414-ad72-d13e511d49d8"
      },
      "source": [
        "Sprawdźmy skąd pochodzi kontekst tej odpowiedzi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93424fe-d03f-490e-8f8f-d85e73d1a7a4",
      "metadata": {
        "id": "b93424fe-d03f-490e-8f8f-d85e73d1a7a4"
      },
      "outputs": [],
      "source": [
        "print(result[\"source_documents\"][0].metadata)\n",
        "print(result[\"source_documents\"][1].metadata)\n",
        "print(result[\"source_documents\"][2].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb86ebe7-c4f7-4ee3-b5f6-ffbd51c7535d",
      "metadata": {
        "id": "bb86ebe7-c4f7-4ee3-b5f6-ffbd51c7535d"
      },
      "source": [
        "Odpowiedź wynika stąd, że w obecnej formie nasz RAG nie ma pamięci. Każde zapytanie jest niezależne i nie możemy prowadzić dyskusji pogłębiającej poprzednie pytania. Zaraz to naprawimy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eca0a2e6-7741-47de-9d10-dcc4818c2908",
      "metadata": {
        "id": "eca0a2e6-7741-47de-9d10-dcc4818c2908"
      },
      "source": [
        "## Pamięć i czat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f02544e-000e-4894-92f3-2f08309eb350",
      "metadata": {
        "id": "3f02544e-000e-4894-92f3-2f08309eb350"
      },
      "source": [
        "Jeśli zależy nam na dłuższych, wieloetpaowych rozmowach z LLMem, będziemy potrzebować pamięci. Pamięc będzie po prostu zapamiętywać zadane pytania i uzyskane odpowiedzi. Poniżej kod tworzący pamięć."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85bf6ce-1eb2-4653-8f2a-613160e26612",
      "metadata": {
        "id": "c85bf6ce-1eb2-4653-8f2a-613160e26612"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "memory.clear() # gdy chcemy zresetować pamięć"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18473c97-b6bd-47e5-9c76-d76f1b3f1cd7",
      "metadata": {
        "id": "18473c97-b6bd-47e5-9c76-d76f1b3f1cd7"
      },
      "source": [
        "Teraz zmienimy łańcuch wywołań na taki korzystający z pamięci. Uwaga, ten chain ma trochę inne nazwy pól zapytania i odpowiedzi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14391427-787c-4a69-8146-923a51e691b4",
      "metadata": {
        "id": "14391427-787c-4a69-8146-923a51e691b4"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "retriever=vectordb.as_retriever()\n",
        "chat_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfc0b7e-58a3-4926-acba-e557a5730797",
      "metadata": {
        "id": "edfc0b7e-58a3-4926-acba-e557a5730797"
      },
      "source": [
        "Teraz ponówmy nasz eksperyment. Zadajmy pytanie i poprośmy o rozszerzenie poprzedniej odpowiedzi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd78038c-9278-4f77-8038-ad75b1003822",
      "metadata": {
        "id": "cd78038c-9278-4f77-8038-ad75b1003822"
      },
      "outputs": [],
      "source": [
        "question = \"What is an eyeball dataset?\"\n",
        "\n",
        "result = chat_chain({\"question\": question})\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29713dbc-7250-4b03-8023-e769b8a37177",
      "metadata": {
        "id": "29713dbc-7250-4b03-8023-e769b8a37177"
      },
      "outputs": [],
      "source": [
        "result = chat_chain({\"question\": \"Can you provide a longer and more detailed response to my last question?\"})\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4116bec4-00db-4d68-a6ca-913bcbaf724c",
      "metadata": {
        "id": "4116bec4-00db-4d68-a6ca-913bcbaf724c"
      },
      "source": [
        "## Ocena systemu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea084b5-767f-4c3c-b16a-bd7a9a949ad3",
      "metadata": {
        "id": "aea084b5-767f-4c3c-b16a-bd7a9a949ad3"
      },
      "source": [
        "Jako ostatni element zapoznamy się z metodami oceny systemów typu RAG. Twórcę takiego systemu może interesować na ile trafne są odpowiedzi, na ile kontekst wspiera odpowiedź i na ile kontekst pasuje do pytania. Te trzy elementy sprawdzają miary Answer Relevance, Groundedness i Context Relevance. Poniżej kod tworzący nowy chain (taki kóry pozwoli zajrzeć do kontekstu i sklei nam wszystkie fragmentu kontekstu w jeden łańcuch znaków. Następnie definicja wspomnienych trzech miar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe73f10-bc6c-42ca-9a78-33b6bfa1ac48",
      "metadata": {
        "id": "9fe73f10-bc6c-42ca-9a78-33b6bfa1ac48"
      },
      "outputs": [],
      "source": [
        "from trulens.core import Feedback, TruSession\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "tru = TruSession()\n",
        "tru.reset_database()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffe916b1-a3d1-4dc4-bb75-689783c26dfe",
      "metadata": {
        "id": "ffe916b1-a3d1-4dc4-bb75-689783c26dfe"
      },
      "outputs": [],
      "source": [
        "from trulens.providers.langchain import Langchain\n",
        "from trulens.apps.langchain import TruChain\n",
        "from trulens.apps.langchain import WithFeedbackFilterDocuments\n",
        "\n",
        "from langchain_google_genai.llms import GoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser\n",
        "import numpy as np\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | QA_CHAIN_PROMPT\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "gemini = GoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        ")\n",
        "provider = Langchain(chain=gemini)\n",
        "\n",
        "context = TruChain.select_context(rag_chain)\n",
        "\n",
        "\n",
        "# Groundedness\n",
        "f_groundedness = (\n",
        "    Feedback(\n",
        "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
        "    )\n",
        "    .on(context.collect())  # collect context chunks into a list\n",
        "    .on_output()\n",
        ")\n",
        "\n",
        "# Question/answer relevance between overall question and answer.\n",
        "f_answer_relevance = Feedback(\n",
        "    provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
        ").on_input_output()\n",
        "# Context relevance between question and each context chunk.\n",
        "f_context_relevance = (\n",
        "    Feedback(\n",
        "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
        "    )\n",
        "    .on_input()\n",
        "    .on(context)\n",
        "    .aggregate(np.mean)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68442d2a-6517-4346-9326-cebc0b37b74a",
      "metadata": {
        "id": "68442d2a-6517-4346-9326-cebc0b37b74a"
      },
      "source": [
        "Teraz stworzymy obiekt `tru_recorder`, który będzie monitorował wszystkie zapytania i je oceniał. Zamiast testować na jednym zapytaniu przeprowadzimy eksperyment i policzymy średnią z 5 zapytań żeby ocenić nasz system."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens.apps.langchain import TruChain\n",
        "from time import sleep\n",
        "\n",
        "def invoke_with_retries(rag_chain, question, max_retries=5): #we are likely to exceed rate limit, that is why we use this function - if the error still occurs limit the number of eval questions\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            with tru_recorder as recording:\n",
        "                return rag_chain.invoke(question)\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                retries += 1\n",
        "                wait_time = 2 ** retries  # Exponential backoff\n",
        "                print(f\"Rate limit hit. Retrying in {wait_time} seconds...\")\n",
        "                sleep(wait_time)\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(\"Max retries exceeded\")\n",
        "\n",
        "tru_recorder = TruChain(\n",
        "    rag_chain,\n",
        "    app_name=\"ChatApplication\",\n",
        "    app_version=\"Chain1\",\n",
        "    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness],\n",
        ")\n",
        "\n",
        "eval_questions = [\n",
        "    'What is an eyeball dataset?',\n",
        "    'What are the keys to building a career in AI?',\n",
        "    'What is the importance of networking in AI?',\n",
        "    'How can altruism be beneficial in building a career?',\n",
        "    'What is imposter syndrome and how does it relate to AI?'\n",
        "]\n",
        "\n",
        "for question in eval_questions:\n",
        "  llm_response = invoke_with_retries(rag_chain, question, max_retries=6)\n"
      ],
      "metadata": {
        "id": "0I1Lc5V7-bWt"
      },
      "id": "0I1Lc5V7-bWt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tru.get_leaderboard()"
      ],
      "metadata": {
        "id": "i8K5_hnj-vhL"
      },
      "id": "i8K5_hnj-vhL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "94d21186-1b34-4ad9-ba5f-1906149fca5a",
      "metadata": {
        "id": "94d21186-1b34-4ad9-ba5f-1906149fca5a"
      },
      "source": [
        "Możemy zobaczyć jak nasz system sobie radzi uruchamiając komendę `tru.get_leaderboard()`. Jeśli będziemy testować wiele wersji aplikacji (parametr `app_id`) to możemy porównywać wersje między sobą właśnie w ramach tabeli wyników."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589676af-c40d-45a0-9db1-22cb7c08cba0",
      "metadata": {
        "id": "589676af-c40d-45a0-9db1-22cb7c08cba0"
      },
      "source": [
        "Na deser: poniższy kod uruchamia dashboard gdzie można przeanalizować każde zapytanie i uzyskane miary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90cc352a-cdc6-437e-bed9-48bb075ee0eb",
      "metadata": {
        "id": "90cc352a-cdc6-437e-bed9-48bb075ee0eb"
      },
      "outputs": [],
      "source": [
        "tru.run_dashboard() #go to records"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a925fd1-7ca7-4fe2-bf3f-4bf2c484fb4d",
      "metadata": {
        "id": "0a925fd1-7ca7-4fe2-bf3f-4bf2c484fb4d"
      },
      "source": [
        "**Zad. 7: Czy jesteś w stanie stworzyć nową wersję RAG, która uzyska lepsze metryki? Spróbuj zmienić prompt, żeby odpowiedzi od LLMa były dłuższe. Spróbuj zmienić liczbę fragmentów wydobywanych z bazy. Spróbuj zmienić długość fragmentów, na które dzielone są dokumenty.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c47519e-487a-4735-aef7-de17abe9475a",
      "metadata": {
        "id": "8c47519e-487a-4735-aef7-de17abe9475a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}